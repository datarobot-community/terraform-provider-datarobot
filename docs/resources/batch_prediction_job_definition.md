---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "datarobot_batch_prediction_job_definition Resource - datarobot"
subcategory: ""
description: |-
  Batch Prediction Job Definition
---

# datarobot_batch_prediction_job_definition (Resource)

Batch Prediction Job Definition

## Example Usage

```terraform
resource "datarobot_batch_prediction_job_definition" "example" {
  name          = "Example Batch Prediction Job Definition"
  deployment_id = datarobot_deployment.batch_prediction_job_definition.id
  intake_settings = {
    type          = "s3"
    url           = "s3://datarobot-public-datasets-redistributable/1k_diabetes_simplified_features.csv"
    credential_id = "${datarobot_basic_credential.batch_prediction_job_definition.id}"
  }

  # Optional parameters
  output_settings = {
    type          = "s3"
    url           = "s3://my-test-bucket/predictions.csv"
    credential_id = "${datarobot_basic_credential.batch_prediction_job_definition.id}"
  }
  csv_settings = {
    delimiter = "."
    quotechar = "'"
    encoding  = "utf-8"
  }
  num_concurrent            = 1
  chunk_size                = 10
  max_explanations          = 5
  threshold_high            = 0.8
  threshold_low             = 0.2
  prediction_threshold      = 0.5
  include_prediction_status = true
  skip_drift_tracking       = true
  passthrough_columns_set   = "all"
  abort_on_error            = false
  include_probabilities     = true
  column_names_remapping = {
    "col1" = "newCol1"
  }
  schedule = {
    minute       = ["15", "45"]
    hour         = ["*"]
    month        = ["*"]
    day_of_month = ["*"]
    day_of_week  = ["*"]
  }
}

output "example_id" {
  value       = datarobot_batch_prediction_job_definition.example.id
  description = "The id for the example batch prediction job definition"
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `deployment_id` (String) The ID of the deployment to use for the batch prediction job.
- `intake_settings` (Attributes) A dict configuring how data is coming from. (see [below for nested schema](#nestedatt--intake_settings))

### Optional

- `abort_on_error` (Boolean) Default behavior is to abort the job if too many rows fail scoring. This will free up resources for other jobs that may score successfully. Set to false to unconditionally score every row no matter how many errors are encountered. Defaults to True.
- `chunk_size` (Dynamic) Which strategy should be used to determine the chunk size. Can be either a named strategy or a fixed size in bytes.
- `column_names_remapping` (Map of String) Mapping with column renaming for output table.
- `csv_settings` (Attributes) CSV intake and output settings. (see [below for nested schema](#nestedatt--csv_settings))
- `enabled` (Boolean) Whether or not the job definition should be active on a scheduled basis. If True, schedule is required.
- `explanation_algorithm` (String) Which algorithm will be used to calculate prediction explanations.
- `include_prediction_status` (Boolean) Include the prediction_status column in the output. Defaults to False.
- `include_probabilities` (Boolean) Flag that enables returning of all probability columns. Defaults to True.
- `include_probabilities_classes` (List of String) List the subset of classes if a user doesn’t want all the classes. Defaults to [].
- `max_explanations` (Number) Compute prediction explanations for this amount of features.
- `name` (String) The name you want your job to be identified with. Must be unique across the organization’s existing jobs.
- `num_concurrent` (Number) Number of concurrent chunks to score simultaneously. Defaults to the available number of cores of the deployment. Lower it to leave resources for real-time scoring.
- `output_settings` (Attributes) A dict configuring how scored data is to be saved. (see [below for nested schema](#nestedatt--output_settings))
- `passthrough_columns` (List of String) Keep these columns from the scoring dataset in the scored dataset. This is useful for correlating predictions with source data.
- `passthrough_columns_set` (String) To pass through every column from the scoring dataset, set this to all.
- `prediction_instance` (Attributes) Defaults to instance specified by deployment or system configuration. (see [below for nested schema](#nestedatt--prediction_instance))
- `prediction_threshold` (Number) Threshold is the point that sets the class boundary for a predicted value. This value can be set between 0.0 and 1.0.
- `prediction_warning_enabled` (Boolean) Add prediction warnings to the scored data. Currently only supported for regression models. Defaults to False.
- `schedule` (Attributes) Defines at what intervals the job should run. (see [below for nested schema](#nestedatt--schedule))
- `skip_drift_tracking` (Boolean) Skips drift tracking on any predictions made from this job. This is useful when running non-production workloads to not affect drift tracking and cause unnecessary alerts. Defaults to false.
- `threshold_high` (Number) Only compute prediction explanations for predictions above this threshold. Can be combined with threshold_low.
- `threshold_low` (Number) Only compute prediction explanations for predictions below this threshold. Can be combined with threshold_high.
- `timeseries_settings` (Attributes) Configuration for time-series scoring. (see [below for nested schema](#nestedatt--timeseries_settings))

### Read-Only

- `id` (String) The ID of the batch prediction job definition.

<a id="nestedatt--intake_settings"></a>
### Nested Schema for `intake_settings`

Required:

- `type` (String) Type of data source.

Optional:

- `catalog` (String) The name of specified database catalog for JDBC type.
- `credential_id` (String) The ID of the credentials for S3 or JDBC data source.
- `data_store_id` (String) The ID of the external data store connected to the JDBC data source.
- `dataset_id` (String) The ID of the dataset to score for dataset type.
- `endpoint_url` (String) Any non-default endpoint URL for S3 access.
- `fetch_size` (Number) Changing the fetchSize can be used to balance throughput and memory usage for JDBC type.
- `file` (String) String path to file of scoring data for localFile type.
- `query` (String) A self-supplied SELECT statement of the data set you wish to predict for JDBC type.
- `schema` (String) The name of specified database schema for JDBC type.
- `table` (String) The name of specified database table for JDBC type.
- `url` (String) The URL to score (e.g.: s3://bucket/key) for S3 type.


<a id="nestedatt--csv_settings"></a>
### Nested Schema for `csv_settings`

Optional:

- `delimiter` (String) Fields are delimited by this character. Use the string tab to denote TSV (TAB separated values). Must be either a one-character string or the string tab.
- `encoding` (String) Encoding for the CSV files.
- `quotechar` (String) Fields containing the delimiter must be quoted using this character.


<a id="nestedatt--output_settings"></a>
### Nested Schema for `output_settings`

Optional:

- `catalog` (String) The name of specified database catalog for JDBC type.
- `create_table_if_not_exists` (Boolean) If no existing table is detected, attempt to create it before writing data for JDBC type.
- `credential_id` (String) The ID of the credentials for S3 or JDBC data source.
- `data_store_id` (String) The ID of the external data store connected to the JDBC data source.
- `endpoint_url` (String) Any non-default endpoint URL for S3 access.
- `path` (String) Path to save the scored data as CSV for localFile type.
- `schema` (String) The name of specified database schema for JDBC type.
- `statement_type` (String) The type of insertion statement to create for JDBC type.
- `table` (String) The name of specified database table for JDBC type.
- `type` (String) Type of output.
- `update_columns` (List of String) A list of strings containing those column names to be updated for JDBC type.
- `url` (String) The URL for storing the results (e.g.: s3://bucket/key) for S3 type.
- `where_columns` (List of String) A list of strings containing those column names to be selected for JDBC type.


<a id="nestedatt--prediction_instance"></a>
### Nested Schema for `prediction_instance`

Required:

- `host_name` (String) Hostname of the prediction instance.

Optional:

- `api_key` (String) By default, prediction requests will use the API key of the user that created the job. This allows you to make requests on behalf of other users.
- `datarobot_key` (String) If running a job against a prediction instance in the Managed AI Cloud, you must provide the organization level DataRobot-Key.
- `ssl_enabled` (Boolean) Set to false to run prediction requests from the batch prediction job without SSL. Defaults to true.


<a id="nestedatt--schedule"></a>
### Nested Schema for `schedule`

Required:

- `day_of_month` (List of String) Days of the month when the job will run.
- `day_of_week` (List of String) Days of the week when the job will run.
- `hour` (List of String) Hours of the day when the job will run.
- `minute` (List of String) Minutes of the day when the job will run.
- `month` (List of String) Months of the year when the job will run.


<a id="nestedatt--timeseries_settings"></a>
### Nested Schema for `timeseries_settings`

Optional:

- `forecast_point` (String) Forecast point for the dataset, used for the forecast predictions. May be passed if timeseries_settings.type=forecast.
- `predictions_end_date` (String) End date for historical predictions. May be passed if timeseries_settings.type=historical.
- `predictions_start_date` (String) Start date for historical predictions. May be passed if timeseries_settings.type=historical.
- `relax_known_in_advance_features_check` (Boolean) If True, missing values in the known in advance features are allowed in the forecast window at the prediction time. Default is False.
- `type` (String) Type of time-series prediction. Must be 'forecast' or 'historical'. Default is 'forecast'.
